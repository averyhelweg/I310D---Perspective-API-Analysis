# I310D---Perspective-API-Analysis

### In this project I analyzed the Perspective API for Bias and patters relating to a comments toxicity score and weather or not the comment was rated toxic by the rater. I was able to start my project by running some brief words and phrases through the API to establish a basis of understanding of how the API worked and to see if there were any patterns in toxicity scores. In my initial analysis I was able to conclude a few different things such as, the tendency of the API to rate uppercase comments with a higher toxicisity score even if it was the same word or phrase, that phrases and words more specifically profanities in the present tense (verbs) tended to be ranked with higher toxicisity. I then decided I was going to test the uppercase hypothesis and conducted a small test where I ran about 14 phrases through the API to see if a comment was liklier to recieve higher toxicity scores if it was in uppercase and based on my results I was able to conclude that the hypothesis is true, words and phrases in all uppercase letters will tend to receive a higher toxicity score than those that aren't even if its the exact same word/phrase. However I will note that this trend appears more on comments that are already negative, for example comments that already have profanities. After I ran the test on my hypothesis I used both rounds of testing that I conducted to establish my toxicity score threshold. I determined my threshold to be 0.68 because in my brief analysis I noticed that most words/phrases that I would consider to be more of a toxic nature scored at 0.68 and above. After selecting a threshold I created a new dataset (included in the repository) and opened it up to explore why there are comments above my established threshold that arent rated toxic by the rater. To do this I used false positives in relation to toxicity score from the top 5 and bottom 5 comments in the dataset. I found that there were 3 comments that displayed false positives (comments where the API rated them high toxicity but they weren't toxic). The first comment was a long entry discussing the etymology of the word bitch. However because it had the word bitch in the entry so many times the API likely didn't understand the context and rated it high toxicity while the human was able to differentiate and didn't rate the comment toxic. The second comment was not rated toxic by the grader however the API rated it above my established threshold of toxicity. I believe this to be because there wasn't much toxicity within the comment, it appeared to be someone just getting mad about the fact that they were being revoked talk page access. The last comment appears to be more of an insult towards wikipedia admins calling them "Hypocritical" and that word itself scores a 0.5 toxicity score, there was absolutly an error with the API on that comment because the comment itself wasn't toxic.

##### I was slightly surprised by the finding about the uppercase words and phrases being more likely to recieve higher toxicity ratings because if its the same phrase just lowercase it still maintains the same overall meaning. I do feel that there are certainly biases in the API such as entries that refrence religion, ethniticy, or controversial topics being rated higher even if there wasn't a negative connotation to the comment. I also think that it's more difficult for the API to process longer entries so those are common spots where there may be either more false negatives or positives in relation to toxicity score and weather or not the comment was rated toxic by the rater. 

#### The LICENSE of this work is an MIT License
